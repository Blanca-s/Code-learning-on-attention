# Learning-on-Attention-code
This is a learning note on the implemention of attention module in Transformer.\
\
The code I use is provided by the Harvard NLP group and the whole ingredient can be found in: https://nlp.seas.harvard.edu/2018/04/03/attention.html \
\
This note is what I had understood the code of Transformer at the beginning of my learning progress of LLM and because I graduated with a bachelor of science degree, I do not have a mature programming skill, thus, the explanation on the original code may be too verbose, however, I think it is very necessary for beginners. \
\
Still, because I wrote them during the progress of my first read of the original code kindly provided by the Harvard NLP group, I did not organize them well. So you may find  the structure of the whole note is in a mess. From my point of view, this note could not tell you the whole story in a very logic way, the most valuable part of it is the explanaion on some programming technical detials of this code. For example, I've noticed that in one blog (https://zhuanlan.zhihu.com/p/398039366) posted on Zhihu ( a Chinese website) has not given a detailed explanation on why the code uses lambda function in the $112^{\text{st}}$ line of the file "attention.py" and when someone mentioned this doubt in the comment area, the poster had no idea why exactly this operation is carried on here. Becuase the use of this lambda funcion here may be a matter of course here. Nevertheless, I have provided a detialed answer on this issue. I think thismay be very important for those beginners.\
\
Also, as a beginner myself, I am very looking forward on the advice offered by authorities, which would be a precious chance for me to improve myself. And if you are also a beginner and you are very confused about some details that I have not mentioned, feel free to ask and that could be my second chance to have a deeper understanding on Transformer. 
